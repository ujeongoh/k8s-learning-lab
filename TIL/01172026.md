# Kubernetes Learning Journey ðŸš€

**Author:** Joy Oh
**Environment:** macOS (Apple Silicon M3) + Multipass (Ubuntu VMs)  
**Tools:** kubeadm, kubectl, containerd, Calico, Nginx Ingress

---

## Phase 0: Infrastructure Setup (Local VMs)
**Goal:** Provision virtual machines on a local MacBook to simulate a bare-metal server environment.

* **Tooling:** Used **Multipass** to create lightweight Ubuntu VMs.
* **Node Specs:**
    * **Master Node (`k8s-master`):** 2 CPUs, 2GB RAM, 10GB Disk
    * **Worker Nodes (`k8s-worker1, 2`):** 1 CPU, 1GB RAM, 10GB Disk
* **Key Configuration:**
    * Enabled primary IP handling for stable communication.
    * Mounted local directories for file sharing between Host (Mac) and VMs.

---

## Phase 1: Cluster Bootstrapping (Kubeadm)
**Goal:** Install Kubernetes components and initialize the control plane.

### 1. Prerequisites (All Nodes)
* **Disable Swap:** Kubernetes requires swap to be disabled for stability.
    ```bash
    sudo swapoff -a
    ```
* **Kernel Modules:** Loaded `overlay` and `br_netfilter` for container networking.
* **Container Runtime:** Installed **Containerd** and configured `SystemdCgroup = true`.

### 2. Control Plane Initialization (Master)
* Initialized the cluster using `kubeadm`:
    ```bash
    sudo kubeadm init --pod-network-cidr=10.244.0.0/16
    ```
* Configured `kubectl` access by copying `admin.conf` to the user's home directory.

### 3. Worker Node Join
* Joined worker nodes to the cluster using the token generated by the master:
    ```bash
    sudo kubeadm join <MASTER_IP>:6443 --token <TOKEN> --discovery-token-ca-cert-hash <HASH>
    ```
* **Troubleshooting:** Learned how to generate new tokens if the original one expires (`kubeadm token create --print-join-command`).

---

## Phase 2: Networking & Architecture
**Goal:** Understand Pod communication, Service Discovery, and External Access (Ingress).

### 1. Architecture & Overlay Network
* **Concept:** Verified the Client-Server model of Kubernetes (`kubectl` -> API Server).
* **CNI Plugin:** Installed **Calico** to establish an overlay network, allowing Pods on different nodes to communicate via private IPs.
* **Test:** Deployed `curl` and `nginx` pods on specific nodes (`nodeName`) to verify cross-node communication.

### 2. Service Discovery (DNS)
* **Concept:** Pod IPs are ephemeral. **Services** provide stable IPs and DNS names.
* **Verification:**
    * Used `nslookup` to resolve Service names (e.g., `my-nginx-service`).
    * Debugged `NXDOMAIN` responses in Alpine-based images, confirming that DNS resolution eventually succeeds via search domains.

### 3. Ingress & Ingress Controller
* **Concept:** Moving away from `NodePort` (opening random ports) to **Ingress** (L7 Domain Routing).
* **Setup:**
    * Installed **Nginx Ingress Controller** (Bare-metal version).
    * Patched the Service to use fixed NodePorts (**30080** for HTTP, **30443** for HTTPS) for consistent local access.
* **Routing Logic:** The Controller routes traffic based on the **HTTP `Host` Header**.
    * **Simulation:** `curl -H "Host: devops.local" http://<NODE-IP>:30080`
    * **Browser:** Modified `/etc/hosts` to map `devops.local` to the Worker Node IP, enabling real browser access.

---

## ðŸ”œ Next Steps: Phase 3 (Workloads)
* Deploying a complete Frontend/Backend stack.
* Managing Persistent Storage (PV/PVC).
* Handling Configurations (ConfigMaps & Secrets).